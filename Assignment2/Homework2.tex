% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand\gldec[2]{
\underset{#1}{\overset{#2}{\gtrless}}
}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{Homework Assignment # 2}
\author{Rushabh Ashok Dharia\\ 
CSCI-B555 Machine Learning} 
\maketitle

\begin{multline*}
    \text{\textbf{Note: }Homework was discussed with Varun Miranda and Bhushan Malgaonkar}
\end{multline*} 
 
\begin{theorem}[Ans]{1}
\begin{align*}
f(x/\alpha) = \alpha^{x}(1-\alpha)^{1-x} \\
\text{Given }\alpha \text{ is selected from a uniform distribution(0,1)}\\
\text{Therefore }f(\alpha) = \frac{1}{1-0} = 1\\
\end{align*}
\begin{align*}
    argmax_{\alpha}(f(\alpha)f(x/\alpha)) &= 1.\prod_{i=1}^{n}\alpha^{x}(1-\alpha)^{1-x}\\
    \text{Taking log}\\
    &= \sum_{i=1}^{n}(x\log(\alpha)+(1-x)\log(1-\alpha))\\
    \text{Taking derivative w.r.t }\alpha \\
    0 &= \frac{1}{\alpha}\sum_{i=1}^{n}x-\frac{1}{1-\alpha}\sum_{i=1}^{n}(1-x) \\
     \frac{1}{\alpha}\sum_{i=1}^{n}x &= \frac{1}{1-\alpha}\sum_{i=1}^{n}(1-x) \\
     \sum_{i=1}^{n}x - \alpha\sum_{i=1}^{n}x &= N\alpha - \alpha\sum_{i=1}^{n}x \\
     \hat{\alpha}_{MAP} &= \frac{1}{N}\sum_{i=1}^{n}x = \bar{x}
\end{align*}
\end{theorem}
\pagebreak
\begin{theorem}[Ans]{2}
\begin{align*}
argmax_{\theta}\prod_{i=0}^{n}\theta e^{-\theta x} \\
\text{Taking log} \\
\sum_{i=0}^{n}\log{\theta} - \theta x \\
\text{Taking derivative w.r.t.} \theta \\ 
\sum_{i=0}^{n}(\frac{1}{\theta}-x_{i}) &= 0\\
\frac{n}{\theta} &= \sum_{i=0}^{n}x_{i}\\
\hat{\theta}_{MLE} &= \frac{1}{\bar{x}}
\end{align*}
\end{theorem}
\begin{theorem}[Ans]{3}
\begin{proof}[a)]
\begin{align*}
\text{For H0,}\\
f_{X}(x) &= f_{N}(\frac{x+s}{a})\\
\text{For H1,}\\
f_{X}(x) &= f_{N}(\frac{x-s}{b})\\
f_{N}(\frac{x+s}{a}) & \gldec{H1}{H0} f_{N}(\frac{x-s}{b})\\
\text{Since }a&=b\\
f_{N}(\frac{x+s}{a}) & \gldec{H1}{H0} f_{N}(\frac{x-s}{a})\\
\frac{1}{\pi (1+(\frac{x+s}{a})^{2})} & \gldec{H1}{H0} \frac{1}{\pi (1+(\frac{x-s}{a})^{2})}\\
1+(\frac{x-s}{a})^2 & \gldec{H1}{H0} 1 + (\frac{x+s}{a})^{2}\\
x^{2}-2xs+s^{2} & \gldec{H1}{H0} x^{2}+2xs+s^{2} \\
0 \gldec{H1}{H0} 4xs\\
\text{Since s$>$0, s is positive}\\
0 \gldec{H1}{H0} x
\end{align*}
\end{proof}
\pagebreak
\begin{proof}[b)] 
\begin{align*}
r(\hat{y} &= \int_{H0}f(x|y=H1).P(y=H1)dx + \int_{H1}f(x|y=H0).P(y=H0)dx\\
&\text{Since priors are equal }P(y=H1) = P(y=H0) =\frac{1}{2}\\
r(\hat{y} &= \frac{1}{2}\int_{H0}f(x|y=H1)dx+ \int_{H1}f(x|y=H0)dx\\
&= \frac{1}{2} \int_{-\infty}^{0}f_{N}(\frac{x+s}{a}) + \frac{1}{2} \int_{0}^{\infty}f_{N}(\frac{x-s}{b})\\
&= \frac{1}{2}\int_{-\infty}^{0}\frac{1}{\pi (1+(\frac{x+s}{a})^{2})} +  \frac{1}{2}\int_{0}^{\infty}\frac{1}{\pi (1+(\frac{x-s}{b})^{2})}\\
&= \frac{1}{2\pi}\int_{-\infty}^{0}\frac{1}{1+(\frac{x+s}{a})^{2}} + \frac{1}{2\pi}\int_{0}^{\infty}\frac{1}{1+(\frac{x-s}{b})^{2}}\\
\text{put }t &= \frac{x+s}{a} \text{ and }u = \frac{x-s}{b}  \\
\text{When }x&=-\infty\text{,  } t = -\infty\text{. When } x=0 \text{,  }t=\frac{s}{a}\\
\text{When }x&=0\text{,  } u = \frac{-s}{b}\text{. When } x=\infty \text{,  }u=\infty\\
r(\hat{y})&=\frac{1}{2\pi}\int_{-\infty}^{s/a}\frac{1}{1+t^{2}}dt + \frac{1}{2\pi}\int_{-s/b}^{\infty}\frac{1}{1+u^{2}}du\\
&=\frac{1}{2\pi}(\tan^{-1}(\frac{s}{a})+\frac{\pi}{2} +\frac{\pi}{2} - \tan^{-1}(\frac{-s}{b}) )\\
&= \frac{1}{2}+\frac{1}{2\pi}(\tan^{-1}(\frac{s}{a})- \tan^{-1}(\frac{-s}{b}))\\
&= \frac{1}{2}+\frac{1}{2\pi}(\tan^{-1}(\frac{s}{a})+ \tan^{-1}(\frac{s}{b}))\\
&\text{(If }a==b\text{)}\\
r(\hat{y}) &= \frac{1}{2}+\frac{1}{\pi}\tan^{-1}(\frac{s}{a})
\end{align*}
\end{proof}
\end{theorem}
\pagebreak
\begin{theorem}[Ans]{4}
\begin{proof}[a)]
\begin{align*}
f_{Y|X}(y|x) &= \frac{f_{X|Y}(x|y).f_{Y}(y)}{f_{X}(x)}\\
f_{Y}(y) &= \alpha e^{-y\alpha}\\
f_{X}(x) &= \int_{0}^{\infty}f_{X|Y}(x|y).f_{Y}(y)dy\\
&= \int_{0}^{\infty} ye^{-yx}.\alpha e^{-y\alpha} dy\\
&= \alpha \int_{0}^{\infty}ye^{-y(x+\alpha)}dy\\
&\text{Using Integration by parts and LIATE Rule}\\
&= \alpha(y\int e^{-y(x+\alpha)})dy - \int (\frac{d(y)}{dy}\int e^{-y(x+\alpha)})dy)dy)\\
&= \alpha[\frac{-ye^{-y(x+\alpha)}}{x+a}-\frac{e^{-y(x+\alpha)}}{(x+\alpha)^{2}}]_{0}^{\infty}\\
&= \alpha[\frac{-e^{-y(x+\alpha)}}{x+a}(y+\frac{1}{x+\alpha})]_{0}^{\infty}\\
&= \alpha[\frac{e^{-y(x+\alpha)}}{x+a}(y+\frac{1}{x+\alpha})]_{\infty}^{0}\\
&= \alpha[\frac{1}{x+\alpha}(0+\frac{1}{x+\alpha})-0]\\
&= \frac{\alpha}{(x+\alpha)^{2}}\\
f_{Y|X}(y|x) &= \frac{ye^{-yx}\alpha e^{-y\alpha}}{\frac{\alpha}{(x+\alpha)^{2}}}\\
&= (x+\alpha)^2.y.e^{-y(x+\alpha)}
\end{align*}
\end{proof}
\begin{proof}[b)]
\begin{align*}
\hat{y}_{MAP} &= argmax_{y} f_{Y}(y).f_{X|Y}(x|y)\\
&= \alpha e^{-\alpha y}y e^{-yx}\\
&\text{Taking log}\\
&=log(\alpha) - \alpha y + log(y) -yx\\
\text{Taking derivative w.r.t. y}\\
0&= 0-\alpha+\frac{1}{y}-x\\
\hat{y}_{MAP} &= \frac{1}{x+\alpha}
\end{align*}
\end{proof}
\begin{proof}[c)]
\begin{align*}
\hat{y}_{MMSE}&=E[y|x]\\
&=\int_{0}^{\infty}yf_{Y|X}(y|x)dy\\
&=\int_{0}^{\infty}y^{2}e^{-(\alpha+x)y}(\alpha+x)^{2}dy\\
&=(\alpha+x)^{2}\int_{0}^{\infty}y^{2}e^{-(\alpha+x)y}dy\\
&\text{Using integration by parts and LIATE Rule}\\
&= (\alpha+x)^{2}(y^{2}\int_{0}^{\infty}e^{-(\alpha+x)y}dy-\int_{0}^{\infty}(\frac{d}{dy}y^{2}\int_{0}^{\infty}e^{-(\alpha+x)y}dy)dy)\\
&=(\alpha+x)^{2}[(y^{2}\frac{e^{-(\alpha+x)y}}{-(a+x)})_{0}^{\infty} + \frac{2}{a+x}\int_{0}^{\infty}ye^{-(\alpha+x)y}dy\\
&\text{[From a)} \int_{0}^{\infty}ye^{-(\alpha+x)y} dy=  \frac{1}{(x+\alpha)^{2}}\text{]}\\
\hat{y}_{MMSE}&=(\alpha+x)^{2}[-0+0+\frac{2}{\alpha+x}.\frac{1}{(\alpha+x)^2}]\\
&=\frac{2}{\alpha+x}
\end{align*}
\end{proof}
\end{theorem}
\begin{theorem}[Ans]{5}
\begin{proof}[a)]
\begin{align*}
P(S=1|V=1) &= P(S=1) \text{ (Since S is independent of V)}\\
P(S=1) &= P(S=1|G=0).P(G=0)+P(S=1|G=1).P(G=1)\\
&= (1-\gamma)\alpha+(1-\beta)(1-\alpha)
\end{align*}
\end{proof}
\begin{proof}[b)]
\begin{align*}
P(S=1|V=0)  &= P(S=1) \text{ (Since S is independent of V)}\\
&= P(S=1|V=1)\\
&= P(S=1|G=0).P(G=0)+P(S=1|G=1).P(G=1)\\
&= (1-\gamma)\alpha+(1-\beta)(1-\alpha)\\
\text{Explanation} &: \text{S is independent of V. Hence value of V doesn't affect S}
\end{align*}
\end{proof}
\end{theorem}
\pagebreak
\begin{theorem}[Ans]{6}
\begin{proof}[a)]
\begin{align*}
P(X_{2} = salmon) &= P(X_{1}=Winter).P(X_{2} = salmon|X_{1}=Winter) + \\
& P(X_{1}=Autumn).P(X_{2} = salmon|X_{1}=Autumn) \\
&= 0.5*0.9+0.5*0.8 \\ 
&= 0.85\\
P(X_{2} = seabass) &= P(X_{1}=Winter).P(X_{2} = seabass|X_{1}=Winter) + \\
& P(X_{1}=Autumn).P(X_{2} = seabass|X_{1}=Autumn) \\
&= 0.5*0.1+0.5*0.2 \\ 
&= 0.15
\end{align*}
\end{proof}
\begin{proof}[b)]
\begin{align*}
P(Winter) &= P(X_{1} = Winter)*P(X_{2} = salmon|X_{1}=Winter)*P(X_{3}=Dark|X_{2} = salmon)\\
&*P(X_{4}=Wide|X_{2} = salmon)+P(X_{1} = Winter)*P(X_{2} = seabass|X_{1}=Winter)\\
&*P(X_{3}=Dark|X_{2} = seabass)*P(X_{4}=Wide|X_{2} = seasbass)\\
&= 0.25(0.9*0.34*0.4+0.1*0.1*0.95)\\
&=0.0323\\
P(Spring) &= P(X_{1} = Spring)*P(X_{2} = salmon|X_{1}=Spring)*P(X_{3}=Dark|X_{2} = salmon)\\
&*P(X_{4}=Wide|X_{2} = salmon)+P(X_{1} = Spring)*P(X_{2} = seabass|X_{1}=Spring)\\
&*P(X_{3}=Dark|X_{2} = seabass)*P(X_{4}=Wide|X_{2} = seasbass)\\
&= 0.25(0.3*0.34*0.4+0.7*0.1*0.95)\\
&=0.0268\\
P(Summer) &= P(X_{1} = Summer)*P(X_{2} = salmon|X_{1}=Summer)*P(X_{3}=Dark|X_{2} = salmon)\\
&*P(X_{4}=Wide|X_{2} = salmon)+P(X_{1} = Summer)*P(X_{2} = seabass|X_{1}=Summer)\\
&*P(X_{3}=Dark|X_{2} = seabass)*P(X_{4}=Wide|X_{2} = seasbass)\\
&= 0.25(0.4*0.34*0.4+0.6*0.1*0.95)\\
&=0.02785\\
P(Autumn) &= P(X_{1} = Autumn)*P(X_{2} = salmon|X_{1}=Autumn)*P(X_{3}=Dark|X_{2} = salmon)\\
&*P(X_{4}=Wide|X_{2} = salmon)+P(X_{1} = Autumn)*P(X_{2} = seabass|X_{1}=Autumn)\\
&*P(X_{3}=Dark|X_{2} = seabass)*P(X_{4}=Wide|X_{2} = seasbass)\\
&= 0.25(0.8*0.34*0.4+0.2*0.1*0.95)\\
&=0.03195\\
&\text{It is most likely to be winter}
\end{align*}
\end{proof}
\end{theorem}
\pagebreak
\begin{theorem}[Ans]{7}
\begin{proof}[a)]
\begin{align*}
\text{To show that, }E[x] &= \sum_{k}\pi_{k}\mu_{k} \\
\text{Given, }p[x] &= \sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})\\
E[x] &= \sum_{k}\frac{\pi_{k}}{\sqrt{2\pi_{k}\sigma_{k}}}\int_{-\infty}^{\infty}xe^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx\\
\text{Let, }x&=x+\mu_{k}-\mu_{k}\\
&=\sum_{k}\frac{\pi_{k}}{\sqrt{2\pi_{k}\sigma_{k}}}[\int_{-\infty}^{\infty}(x-\mu_{k})e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx+\int_{-\infty}^{\infty}\mu_{k}e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx] &- [1]\\
\text{For }&\int_{-\infty}^{\infty}(x-\mu_{k})e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx\\
\text{Put }& e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}=t\\
\frac{dt}{dx} &= -\frac{x-\mu_{k}}{\sigma_{k}}.e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}} \\
-\sigma_{k}dt &= (x-\mu_{k})e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx\\
\int_{-\infty}^{\infty}(x-\mu_{k})e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx &= -\sigma_{k}\int_{-\infty}^{\infty}dt\\
&= -\sigma_{k}[t]_{-\infty}^{\infty}\\
&= 0 &- [2]\\
\text{Now }\int_{-\infty}^{\infty}\mu_{k}e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx&= \mu_{k}\int_{-\infty}^{\infty}e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx\\
&=\mu_{k}.\sqrt{2\pi_{k}\sigma_{k}}\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi_{k}\sigma_{k}}}e^{\frac{-1}{2}(\frac{x-\mu_{k}}{\sigma_{k}})^{2}}dx\\
&=\mu_{k}.\sqrt{2\pi_{k}\sigma_{k}}(1) & -[3]\\
&\text{Substituting [2] and [3] in [1]}\\
E[x]&=\Sigma_{k}\frac{\pi_{k}}{\sqrt{2\pi_{k}\sigma_{k}}}(0+\mu_{k}.\sqrt{2\pi_{k}\sigma_{k}}(1))\\
&=\Sigma_{k}\pi_{k}\mu_{k}
\end{align*}
\end{proof}
\begin{proof}[b)]
\begin{align*}
cov[x] &= cov[x,x^{T}]\\
&= E[(x-\mu_{x})(x-\mu_{x})^{T}]\\
&= E[xx^{T}-x\mu^{T}-\mu x^{T}-\mu\mu^{T}]\\
&= E[xx^{T}]-\mu_{x}^{T}E[x]-\mu_{x} E[x]^{T}+\mu_{x}\mu_{x}^{T}\\
&= E[xx^{T}]-\mu_{x}^{T}\mu_{x}-\mu_{x} \mu_{x}^{T}+\mu\mu^{T}\\
\text{Therefore, }cov[x]&=\Sigma = E[xx^{T}]+\mu_{x}\mu_{x}^{T}\\
\text{Therefore, }E[xx^{T}] &=\Sigma -\mu_{x}\mu_{x}^{T}\\
\text{For k Gaussians, }E[x] &= \Sigma _{k}\pi_{k}\mu_{k}\\ 
\text{Therefore, }E[XX^{T}] &= \Sigma _{k}\pi_{k}(\Sigma_{k} -\mu_{x}\mu_{x}^{T})\\
\text{Finally, }cov[x] &= E[xx^{T}]- E[x]E[x]^{T}\\
\text{Therefore, }cov[x] &= \Sigma _{k}\pi_{k}(\Sigma_{k} -\mu_{x}\mu_{x}^{T})- E[x]E[x]^{T}
\end{align*}
\end{proof}
\end{theorem}
Ans 8. b)\\
As shown in the plots notebook attached $\sigma$ decreases as number of Gaussians increases. \\
This is because the probability of a point lying in each Gaussian decreases with increase in number of G.\\
Also, we can see that 2 Gaussians in k=4 almost overlap as we are trying to over fit the GMM.\\
Also, the $P(c_{i})$ for each Gaussian decreases with increase in number of Gaussians

% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}